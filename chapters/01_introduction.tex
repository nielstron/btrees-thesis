% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.


\chapter{Introduction}\label{chapter:introduction}

An important topic in computer science is the study of the
\textit{functional correctness} of an algorithm.
It states whether an algorithm satisfies the a specified
input/output behavior.
This topic becomes especially interesting when a) it can be applied
to directly executable code and b) it is machine checked.
Unfortunately with both desirable additions, the tasks becomes
significantly more complex.
Even in an abstract specification, where topics such as
memory management may be abstracted away,
machine checked proofs of non-trivial properties
are hard.
Furthermore for concrete implementations,
low-level decisions about memory allocation or
the eligibility of reusing variables need to be
reasoned about.
In this thesis we provide a computer assisted proof for the functional
correctness of an imperative implementation of the B-Tree data-structure
and present how we dealt with the above mentioned issues.

In \Cref{chapter:introduction}, we have a brief overview on related
work and introduce common variations of the definition of B-Trees.
We choose to implement the definition that is most promising for our approach.
We first design a functional, abstract implementation.
Together with a proof of its functional correctness,
it is presented in \Cref{chapter:abs-set}.
On this level, functional correctness is equivalent with the
implementation of an abstract Set-interface,
that requires retrieval, insertion and deletion operations.
From there, an imperative implementation is derived in \Cref{chapter:imp-set}.
Its functional correctness is shown by proving that it refines the functional specification.
Thus the proof obligation for the imperative implementation
is reduced to a proof of equivalence between the output of the
functional and the imperative implementation.
This gives freedom for low-key optimizations with regard to a naive translation.
Finally, we present learned lessons, compare the results with related work and suggest potential future
research in \Cref{chapter:conclusion}

% TODO related work?
% mfp/fielding: not mechanized, different approaches
% Ernst: more automation, but actually similar approach to ours (except for used structure)
% Malecha: less automation, similar structure


\section{The Isabelle Proof Assistant}

Isabelle/HOL is an interactive theorem prover that allows
to reason, among other things, in Higher Order Logic.\parencite{DBLP:books/sp/NipkowPW02}
It is built in ML, which influences the syntax of functional
programs written in it.
Isabelle code is portioned in so called theories,
roughly being equal to a module in common languages.
A theory consists of the specification of datatypes,
typed functional programs and theorems with proofs.

\subsection{Notation and proofs in Isabelle}

Functions, predicates and the like are all expressed in
a functional manner.
The keyword \textbf{definition} denotes classical definitions
and \textbf{fun} is used for recursive definitions.
The latter supports definition by pattern matching and
is only a valid definition if it incorporates a proof of termination.
Usually this proof is derived automatically by the system.
If a termination can not be guaranteed for all inputs,
a function may be defined via \textbf{partial\_function}.
This is specifically the case for imperative programs
that recurse on pointers, as circle free-ness cannot be guaranteed here.
Partial definitions come at the inconvenience of no
supported pattern matching \parencite{isabelleManual}.
The term \textbf{abbreviation} is used to define simple shorthands for more complex expressions,
similar to a macro.

Lemmas, theorems and the like can be expressed in a similar manner
as they would be written in mathematical textbook.
They begin with \textbf{lemma}, \textbf{theorem} or \textbf{corollary},
followed by an expression or predicate and a proof.
Wherever possible and readable, we will try to reflect the actual
syntax of the Isabelle system, however compromise in order to
keep the notation of obtained lemmata and theorems
close to conventional notation.
To prove a theorem correct, the system basically provides two different
proof styles.

One option is to write structured proofs in the isar language.
The user outlines a proof with intermediate goals
and tells the system which proof methods to apply to
resolve each step.
This method is usually preferred as it is more readable and usually
faster on the system side.
All complex proofs in \Cref{chapter:abs-set} are hence written in this style.

In the apply style, the user tells the system which proof method
to apply to modify the current goal.
This is practical if the number of assumptions
is high or the goal is large and writing the full terms
out would be impractical.
Since this is the case for the proofs of the imperative programs,
almost all proofs in \Cref{chapter:imp-set} are written in this style.
In this work we will not present the proofs as written in the actual
proof files but rather outline the structure of the proofs.

The system provides a number of proof methods,
based on different manipulation tools
such as logical reasoning or simplification.
When mentioning \textit{automatic} proofs, we mean a proof that
comprises very few ($\le 5$) apply style
invocations of proof methods is meant.
A method that commonly allows for such proofs is the \textit{auto} method,
a combination of logical reasoning and repeated simplification,
but also allowing automatic case distinctions and destructive rule application.

\subsection{Examples and basics of the Isabelle language}

Datatypes may be defined recursively.
The following shows as an example the internal definition of the list datatype.\footnote{
    The actual definition is worded slightly different but this is of no importance here.
}

\begin{lstlisting}[mathescape=true, language=Isabelle,label=lst:list-def]
datatype 'a list = [] | 'a # 'a list
\end{lstlisting}

In natural language this means that either a list is the empty list,
or it is an element prepended on another list.
As usual in ML like languages, we may deconstruct an argument to a function
by pattern matching to this construction.
In addition, the type $'t$ of any expression $e$ can be
made explicit by writing \textit{e :: 't}.
Functions that take values of type $'a$ and return values of type $'b$ 
have type \textit{'a $\Rightarrow$ 'b}.
The Isabelle internal list catenation function serves as
an example for a function definition with explicit type.

\begin{lstlisting}[mathescape=true, language=Isabelle,label=lst:append-def]
fun (@) :: 'a list $\Rightarrow$ 'a list $\Rightarrow$ 'a list where
    [] @ ys = ys |
    (x#xs) @ ys = x # (xs @ ys)
\end{lstlisting}

% TODO include?
% locales
% set specification
It is possible to specify functions, predicates
and theorems with respect to some abstracted function
of which only certain properties are known,
resulting in so-called \textbf{locales}.

Further details on notation, proof techniques and more in Isabelle/HOL
may be found in Chapter 1 of \parencite{DBLP:books/sp/NipkowK14}.


\section{The B-Tree Data Structure}

B-Trees were first proposed by Bayer et al. in \parencite{DBLP:journals/acta/BayerM72},
as a data-structure to efficiently store and retrieve indices stored on storage devices
with slow memory access.
They are $n$-ary balanced search trees, where each
node contains many indices.
The number of subtrees and indices in each node is
defined by the \textit{order} of the tree.
% TODO reorder with definitions

B-Trees are a generalization of 234-Trees and a specialization of (a,b)-Trees

A commonly implemented variation is the B$^+$-Tree, where the inner nodes
only contain separators to guide the recursive search
but all data is stored in the leaves \parencite{DBLP:journals/csur/Comer79}.
Further the leaves are often implemented so as to contain pointers
to the next leaf in order, allowing for efficient range queries.
Also usually B-Trees are supposed to implement a map-like
structure, where each element is actually an index together with a pointer
or some arbitrary additional data.

However we will not consider these more implementation centered
additions and first show the validity of our approach
with a more abstract variant of the B-Tree.
We say that B-Trees store some linearly ordered values
that could be stored in an abstract set, and the B-Tree is the concretization
of this set.

\subsection{Definitions}
\label{sec:data_structure_defs}

Every node contains a list of \textit{keys} (also \textit{separators}, \textit{index elements}), and \textit{subtrees} (\textit{children}),
that refer to further B-Trees.
The separators and subtrees may be considered interleaved within a node,
such that we can speak of a subtree left of a separator and a subtree right of a separator,
where for a separator at index $i$ we mean the subtree in the respective
subtree list at index $i$ and $i+1$ respectively.
Note that this already implies that the list of subtrees is one
longer than the list of separators - we refer to the last subtree
as the \textit{last} or \textit{dangling} subtree.
In \parencite{DBLP:journals/acta/BayerM72},
a B-Tree with above structure must fulfill the three properties
\textit{balancedness}, \textit{order} and \textit{sortedness}.

\paragraph{Balancedness} \textit{Balancedness} requires
that each path from the root to any leaf has the same length $h$.
In other words, the height of all trees in one level of the tree must be equal.
This is possible to accomplish due to the flexible amount of subtrees
in each node.

\paragraph{Sortedness} Further the indices must be \textit{sorted} within the tree which means that all indices stored
in the subtree left of a separator are smaller than the value of the separator
and all indices on the right are greater.
Further all indices within a node should maintain a sorted order.

\paragraph{Order} In general terms, the property of \textit{order} ensures a certain minimum and maximum
number of subtrees for each node.
However, as pointed out in \parencite{DBLP:books/daglib/0095349_mod},
the property is defined differently in the literature.
For the purpose of this work, the original definition by Bayer et al. was chosen as most suitable.
A B-Tree is of order $k$, if each internal node has at least $k+1$
subtrees and at most $2k+1$.
The alternative of defining the nodes to have between $\lceil \frac{k}{2} \rceil$
and $k$ children (as proposed in \parencite{DBLP:books/lib/Knuth98a}),
involves cumbersome \textit{real} arithmetic that unnecessarily complicates
mechanized proofs.
Sticking to the original definition is further supported by the fact that nodes are supposed
to fill memory pages which are usually of even size (usually some power of 2).
An even number of separators and trees plus one dangling last right tree maximizes
the usage of such a page.

The same ambiguity exists for the term \textit{Leaf} which we will define consistently with Knuth's definition \parencite{DBLP:books/lib/Knuth98a}
to be an empty node, carrying no information,
rather than a node without children.
This is close to the usual approach in functional programming,
and will yield elegant recursive equations for our B-Tree operations.
The lowest level of nodes hence contains a list of separators and
list of pointers to leaves.

Note that the B-Tree definition is only meaningful for positive $k$.
For the case that $k$ is equal to 0,
all elements of the tree would have exactly one subtree
and no internal elements.
For the root node this even leads to a contradictory state:
It is required to contain at least 1 element.
However as it should not have more than $2k = 0$ elements,
this constraint cannot be satisfied.
Requiring positive $k$ is consistent with the definitions
in the literature consulted \parencite{DBLP:journals/acta/BayerM72,DBLP:journals/csur/Comer79,DBLP:books/daglib/0023376}

% insert image of valid B-Tree
A simple example B-Tree may be seen in \Cref{fig:btree-basic-nopair}.
Nodes and Leafs are represented as circles, were the former points to the
subtree that a node represents and the latter is left empty.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/btree-basic-nopair.pdf}
    \caption[A small example B-Tree]{A small balanced, sorted B-Tree of order $2$ and
    height $2$ containing $3$ nodes and $6$ elements.
    Circles represent nodes, where empty circles are leafs and filled circles
    point to the corresponding subtree.}
    \label{fig:btree-basic-nopair}
\end{figure}

\subsection{B-Tree operations}

The B-Tree is a dynamic data-structure and provides
a number of operations to inspect or modify the stored data.
Generally the operations are defined recursively on the nodes.
The correct subtree may be found by inspecting the separators stored
in the currently visited node.
If the value that is being searched for is in the range of two
adjacent separators, the tree in between is the correct for recursion. 
The obvious corner cases are if the value is less than the
minimal element or greater than the maximum element stored.
In that case we recurse in either the first or the last subtree.
The exact manner of inspection should in practice of course
be efficient and will hence be kept abstract until \Cref{chapter:imp-set}.

\paragraph{Retrieval}\label{par:intro-isin}
Since the whole tree is sorted,
checking whether certain elements are contained in the tree
is simply conducted by recursing into the correct node
in each level.
Either the element is found directly or found at a lower level.
If we reach a leaf node we know that the element is not contained in the tree.
There is little variation on this algorithm so there is no need for comparison.

% TODO move abstract discussion to introduction
%-------------------------------------------------
\paragraph{Insertion}\label{par:intro-ins}
There is also much consensus in the literature on how to conduct insertion into a B-Tree.
\parencite{DBLP:journals/csur/Comer79}
%TODO citations
%TODO relationship to other implementations (are there more?)
Generally, an element is inserted into the nodes on the lowest level.
In case that there is enough space left, the element is simply placed at the correct
position in the list of separators.
However if the node has more than $2k$ elements after this insertion,
we need to split it and, passing the median to the parent node,
recurse back upwards.
We will see in \Cref{sec:abs-ins} how this can be
elegantly expressed in a functional specification.
%-------------------------------------------------

% TODO delete
% TODO move abstract discussion to introduction
%-------------------------------------------------
\paragraph{Deletion}\label{par:intro-del}
On deletion, elements are removed from the leaves only.
If the element to be deleted resides in an inner node,
it is replaced by the maximal lesser or minimal greater
element in the tree, which always resides in a leaf
to the left or right of the element to be removed.

After deletion, the nodes may need rebalancing in order
to ensure the order property.
A node having less than $k$ elements is called to have \textit{underflow}.
However, opposing to the insertion function,
the exact procedure to handle underflow varies strongly in the literature.
Since only one element is removed from the node,
the most intuitive remedy to underflow is to \textit{steal} or \textit{borrow} it
from the left or right sibling.\parencite{DBLP:books/daglib/0023376}
If the left or right sibling has more than $k$ elements,
one of the neighboring elements may be moved out for increasing
the size of the current node.
Only in case that both siblings have left only $k$ elements,
a kind of reversal of the insertion split is conducted:
One of the siblings and the node itself are merged to form
a new, bigger node of valid order.

Following the description of \parencite{DBLP:journals/acta/BayerM72},
as done by \parencite{Fielding80},
the two cases can be treated identically.
If a node has less than $k$ elements,
merge it with one of its siblings.
If the resulting catenated node has an overflow again,
it will be split in half, just as with insertion related overflows.
According to \parencite{DBLP:journals/csur/Comer79} this may even be
more efficient than stealing single single keys from siblings.
First of, the resulting node is less likely to underflow again.
If it had stolen only one item, the probability for another underflow at the next
deletion is higher than if several elements are copied over.
Further, the node to merge with has to be read from memory anyways.
The cost of moving around up to $k$ elements that already reside in memory
is hence negligible compared to the cost of another underflow.

%-------------------------------------------------

\subsection{Properties}

B-Trees are assumed to be stored on external memory,
such that each node roughly matches a page in main memory.
Due to the potentially large branching factor and the balancing,
the number of required memory accesses for retrieving data stays small even for
large amounts of data stored.
This is due to the fact that the overall number of memory accesses is bounded by the depth
of the tree, which again is logarithmic in the number of indices -
where the base of the logarithm is closely proportional to the order $k$ of the tree.

Further, by design, the storage usage of B-Trees is at a minimum close to $50\%$,
where the average usage is usually higher. \parencite{DBLP:journals/acta/BayerM72}
This is due to the fact that every node reserves the storage of $2k$ keys and separators
but by definition always contains at least $k$ elements.

B-Trees build the foundation to most modern relational database implementations.
The above mentioned properties are key to the widespread popularity of B-Trees and are
hence preserved in the given implementation.
% TODO more

\section{Related Work}

All found related work considers B$^+$-Trees
rather than B-Trees, however some parallels can be found.

There exist two pen and paper proofs via the rigorous approach
by Fielding \parencite{Fielding80} and Sexton \parencite{DBLP:journals/entcs/SextonT08}.
Even though not machine checked, they shed light on techniques applied in this work.

Fielding approaches the verification by refinement.
First, B$^+$-Trees are viewed as nested sets of subtrees
or as leafs that are sets of key-value pairs.
Obtaining the correct subtree for recursion is in the abstract setting
only defined by appropriate pre and post-condition
and an actually executable function is provided in the second refinement step.
On this level, only arguments on the invariants are made.
In a second step B-Trees are considered more concretely to contain
sorted lists of children and keys.
Here the argument for invariant preservation is due to the implementation
having the same structure as the implementation in the abstract context.
Final imperative PASCAL code is given, derived by hand
imitating the functional style implementations and extended by assertions.

This approach is similar to what will be done in this work,
however mostly concerning the methodology rather than the actual
steps of refinement.
In this work, we will start at the definition using lists in \Cref{chapter:abs-set}
and refine to an imperative implementation with pointers and arrays in \Cref{chapter:imp-set}.
The code we obtain will be exported automatically to a functional language of choice.

We reason about the validity of this refinement via separation logic,
the same tool as employed by Sexton \parencite{DBLP:journals/entcs/SextonT08}.
They show in their work that this tool allows to reason based on some kind of locality,
in particular that operations on subtrees are really only affecting subtrees.
We will make implicit use of this property in \Cref{chapter:imp-set}
for proofs on our imperative implementation.

The specification by Sexton is given as an abstract machine rules,
somewhat more abstract than pure code but operating on a stack
much like we expect imperative programs to operate with pointers on a heap.
We therefore categorize this as a direct proof on an imperative implementation.

Of these imperative implementations, also two machine checked proofs exist.
In the work of Ernst \parencite{DBLP:journals/sosym/ErnstSR15},
an imperative implementation is directly verified
by combining interactive theorem proving 
with shape analysis.
The main recursive procedures are interactively verified in KIV.
Data structure properties such as circle-freeness are then proven by shape-analysis.
Another direct proof on an imperative specification 
was conducted by Malecha \parencite{DBLP:conf/popl/MalechaMSW10}, with the YNOT
extension to the interactive theorem prover Coq.
Both works use recursively defined "shape" predicates
that describe how the nodes and pointers
form a bounded and circle free abstract tree.
However, we know of no verification that explicitly covers
the implementation of functional B-Trees.
In addition to providing one, this work aims to show the benefits
of taking this indirection for imperative implementations.
That is that the analysis of invariants of an abstract shape may be spared
or at least significantly simplified when
restricting the concrete pointer structures to be refinements
of an abstract algebraic type.