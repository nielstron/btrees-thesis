% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.


\chapter{Conclusions}\label{chapter:conclusion}

In this work, we obtained an imperative implementation
of B-trees that is capable to handle membership and insertion queries.
The implementation can be supplied with different
intra-node search algorithms, of which a linear search based
and a binary search based version are included.
Concrete code has been obtained in the languages Scala and SML,
where the code generator may simply be extended to support other languages.
We have proven the membership and insertion operations
functionally correct within the Isabelle framework
with the help of an abstract specification
that is refined by the imperative code.
The abstract specification fully implements
an abstract set interface on linearly ordered elements,
also supporting deletion queries.
The Isabelle proof documents can be found
in the appendix \parencite{MuendlerAppendix21}.

\section{Working in Isabelle}

An overview on the resulting theory modules,
their relationship to external modules and one another
can be seen in \Cref{fig:thm-relationships}.
The given graphic roughly resembles the actual file structure.
It can be seen that in order to develop this project
not many general lemmas and definitions had to be added to the library.
The library of proofs considered here
is the one provided by the Isabelle/HOL framework together with the \href{https://www.isa-afp.org}{\textit{Archive of Formal Proofs}}.
The latter is a refereed collection of proofs that contains, among others,
the separation logic framework.

\begin{figure}
    \centering
    \includegraphics[width=1.1\linewidth]{figures/theories-relationship.pdf}
    \caption[An overview on the theories comprised by this work
    and their approximate relationship.]{
    An overview on the theories comprised by this work
    and their approximate relationship.
    An arc points to a theory or package
    that contains required lemmas or definitions.
    Theories created or significantly extended in the course of this work are outlined with a continuous line.}
    \label{fig:thm-relationships}
\end{figure}

Overall, proving in the Isabelle framework is quite comfortable
as it supplies the user with a plethora of tools
to find required or related lemmas,
or even find proofs of whole theorems.
However there were of course small issues
in the usage of the tool that tend to be 
inherent to machine checked proofs.

One main source of frustration was finding the correct
library functions and lemmas for smooth proofs.
In informal proofs, a certain kind of function and corresponding equalities can be
expected to be known by the reader.
However for machine checked proofs
all used functions and lemmas need to be stated
and proven explicitly.
Therefore using library functions that are supplied
with a sufficient amount of lemmas is vital for proofs
to require acceptable time and effort.
The Isabelle/HOL framework provides tools like sledgehammer
or \textbf{find\_theorems} that usually find useful lemmas for the concrete situation.
However each have their own drawbacks.

If the goal is only to simplify a given expression but not to
directly solve the result, sledgehammer is useless
as it only returns the applied lemmas for a successfully finalized proof.
There is no feedback on "how far" the proof got.
Lemmas that could be applied to significantly simplify the goal
but did not yield a complete proof are discarded.
Showing what was applied
would have been especially useful for the apply style proofs of the
imperative implementation that was mainly comprised of simplification steps.
Maybe the tools can be extended to return what lemmas lead to promising
simplifications of the the goal term to provide additional input to the user.

Secondly \textbf{find\_theorems} is useless if the current state
is simplified too much to perfectly match existing lemmas.
As a concrete example, the $inorder$ of B-trees was first defined
with "foldr map \textit{$\dots$ ts []}".
The proof system could not provide any useful lemmas on expressions of this form.
However there exists the standard function $concat$ that
simplifies to the above.
The standard library contains useful lemmas for applications of this function.
The function was only discovered close to the finalization of this thesis,
in a phase where standard terms simplifying the stated functions were looked for.
Since the available lemmas were key to a practical
implementation of the inorder set interface,
this meant a wasted workload of almost 2000 lines of additional proof.
The lesson learnt is to always look for alternative or equivalent
formulations of properties as well as lemmas,
to see what kind of useful lemmas may already exist in the standard library.

Another point of discomfort is in the separation logic framework.
Especially when inserting elements in lists,
original assumptions on the whole list have to be split
into assumptions on parts of the list or lists obtained by concatenation.
The aggressive simplification of the automatic
separation logic prover (\textbf{sep\_auto})
does often go to far in these proofs.
This either results in unreadable (and unprovable)
subgoals resulting from a chain of commands.
Sometimes it also yields states for which a rule
may not be applied anymore.
For example when we apply $node_i$ on some list,
this list has usually been changed to append or insert a single element.
This means that $blist\_assn$ holds for the whole list.
However the automation tools knows that the list
is comprised on sublists and automatically simplifies related expressions.
In order to match the resulting precondition with the $node_i$ heap rule
defined on full lists, an intermediate rule has to be derived
that has as precondition this simplified state.


% TODO note that node_i requires temporary allocation of an array that contains all elements.
% it would be more efficient to derive a specialized version that takes two arrays
% and redistributes between them evenly


\section{Comparison of approaches and lessons learnt}

In the abstract analysis part of this work,
the proofs have been written to use only
minimal requirements on the input.
For example in \Cref{lem:splitmax-inorder}, 
we do not require balancedness and $\order k$ for $k > 0$ on the full tree.
Instead, it is only required that the node has at least two children,
the last two have equal height
and that the last tree has the same, recursive property.
Using this more specific assumption was beneficial during the proof,
as the less restrictive requirements turn up in the
induction hypothesis again and are easier to satisfy than general assumptions.
However this also called for more lemmas that show that
the weaker property is satisfied in valid B-trees.
Nevertheless, generating the most general lemmas usually was highly useful.
A good example is \Cref{lem:nodei-order},
where generalizing for up to $4k+1$ input elements yielded the reusability of
$node_i$ for merging two nodes in deletion.
Another use case was the applicability of weaker properties
even when temporary violations
of the general invariant were observed during deletion,
such as in the \textit{del} function in \Cref{lst:del-def}.

% TODO note that the proof for  sortedness has been reduced from 2000 lines
% by the sorted (inorder ...) notation (and Nipkows auxiliary funs/lemmas)
% comparison standard approach and list approach
During this work, a significant difference in the effort
to prove invariants using the standard set interface
and the set interface by inorder has emerged.
The proof of the standard set interface required a number of additional functions
comprising 36 lines of code.
The bigger part however is made up by the proofs required.
Including additional auxiliary lemmas, the whole proof of sortedness
and correct set operations required a staggering over 1900 lines of proof.
The proof was complex and hardly readable due to the 
extensive use of efficient solvers.
This is opposing 5 lines for the inorder and sorted function and
483 lines of proof required for the inorder approach,
which is mostly composed of easily understandable chains of equations.
Of course the invariance preservation proof
was overall simplified by the detour via the functional specification.

% TODO comparison with other approaches
We realize that the rigorous verification of B-trees or the related B$^+$-trees
is a hard problem.
Both Malecha \parencite{DBLP:conf/popl/MalechaMSW10} and Gidon \parencite{DBLP:journals/sosym/ErnstSR15}
report the validity of the tree shape
and invariants such as node size or sorting
to be the most difficult properties to prove.
In our approach, a valid tree shape was directly given
by the refinement property of the algebraic B-tree type.
The other abstract invariants could be reasoned about
in the context of the abstract specifications and as such
required a number of intermediate, but obvious
lemmas that were comparatively straightforward to obtain.
The comparison between the three proof approaches in \Cref{fig:proof-comparison}
shows differences both with respect to the amount of 
time invested and the amount of manually provided lines of proof.
    
\begin{figure}
    \centering
    \begin{tabular}{l|c|c|c}
        \                & \parencite{DBLP:conf/popl/MalechaMSW10}$^{+}$ & \parencite{DBLP:journals/sosym/ErnstSR15}$^{+d}$ & Our approach \\
        \hline
        Functional code &   360      & -                    & 324  \\
        Imperative code &   510      & 1862                  & 170  \\
        Proofs          &  5190      & 350 + 510 + 2940\footnotemark[7] & 2753 \\
        Timeframe (months) &  -     & 6+                      & 4   \\
    \end{tabular}
    \caption[Comparison of (unoptimized) Lines of Code and time investment in related mechanized B-tree verifications.]
    {Comparison of (unoptimized) Lines of Code and time investment in related mechanized B-tree verifications.
    The marker $^+$ denotes implementations of B$^+$-trees
    and $^d$ denotes that the implementation additionally verifies deletion operations.
    }
    \label{fig:proof-comparison}
\end{figure}
\footnotetext[7]{
    The proof integrates TVLA and KIV, and hence comprises
    explicitely added rules for TVLA (the first number),
    user-invented theorems in KIV (the second number)
    and "interactions" with KIV (the second number).
    Interactions are i.e. choices of an induction variable, quantifier instantiation
    or application of correct lemmas.
    We hence interpret them as each one apply-Style command and hence
    one line of proof.
}

In addition to the effort related benefits, the abstract formalization allowed to
verify the implementation of a binary split based node navigation
algorithm that was not yet implemented in any other verified approach.
It seems that the detour via a rigorously implemented
functional implementation yields less complex proofs.
It should however be noted that the B-tree implementation
might in itself be simpler to reason about than B$^+$-trees,
which includes several optimizations regarding
storage of nodes and available pointers.

\section{Future work}

It should be noted that further,
in contrast to related work and common implementations,
this work implements B-trees as concretizations of sets.
Usually B-trees are rather in use as map data structures,
mapping the unique primary keys
of database tuples to stored data.
Future work could adjust the code and proofs given here
to show that similar operations on a similar structure satisfies
the map interface.
The main difference would then be that not the indices
are the main elements of the nodes but pairs of the 
index and the corresponding data or a similar
implementation of a singleton map.
We expect this to be not more than manual work,
but to cause no specific new issues.

It should be noted at this point that the obtained code is not yet
truly optimized with respect to minimal memory allocation and access.
For that, some further refinements are necessary.
For example, the imperative \textit{node$_i$} function
requires temporarily allocating
an array that has enough space to contain
all elements that are supposed to be merged or split.
Optimizing it would maybe give a function that takes
as an argument two arrays and redistributes elements between them,
or an array and an element that is to be inserted.
This naturally increases the required amount of additional,
specialized heap rules and makes the proofs more complex.
How to gradually improving the efficiency of the implementation
without greatly increasing the complexity of correctness
proofs is a topic for further research.


However it should be noted that a transfer of the results in
this work may be difficult for the related B$^+$-tree,
providing another subject for future research.
B$+$-trees usually contain pointers to the next leaf
on the lowest level, a property that is potentially harder
to represent on a functional level than in imperative code.
It might be necessary at that point to introduce additional
invariants directly on the imperative level.