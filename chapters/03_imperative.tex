% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Imperative B-Trees in Isabelle}\label{chapter:imp-set}

In the previous chapter, we have seen
an abstract definition of B-trees and the reasoning
behind its correct implementation of the set interface.
However, the specification would not yield 
efficiently executable code.
% main difference: specify allocation of data
The reason is that the abstract specification works with
the persistent datatypes of nodes and lists.
This way, trivially, no data is unknowingly
modified or corrupted,
however this is at the cost of computational efficiency
as changes require allocating memory for a completely new object.

In order to obtain an efficient implementation
on ephemeral data structures common to imperative languages,
we specify imperative code and show that
it refines the abstract specification.
The imperative code can then be translated to
the languages Scala \parencite{OderskyScala} and SML \parencite{DBLP:books/daglib/0069232}
using the code generation
facilities in Isabelle/HOL \parencite{Haftmann20codegeneration}.
However, code extraction is not limited
to these specific languages
since the code generator is extensible.

\section{Refinement to Imperative/HOL}

The proofs of correct behavior for our imperative
code are stated in separation logic
an extension of Hoare Logic invented by Reynolds \parencite{DBLP:conf/lics/Reynolds02}.
It was formalized in Isabelle/HOL by Lammich \parencite{DBLP:journals/jar/Lammich19}
and comes with a framework that simplifies reasoning about
its expressions.

\subsection{Separation Logic}

Separation logic provides a way to reason about mutable resources
that lie in separated parts of an external heap,
an abstracted memory device.
The assumptions on the state of the heap are called \textit{assertions}.
They are stated as formulae that hold for specific heap states.
The basic assertions used in this work are the following:

\begin{itemize}
    \item \textit{emp} holds for the empty heap
    \item \textit{true} and \textit{false} hold for every and no heap respectively
    \item $\uparrow(P)$ holds if the heap is empty and predicate $P$ holds
    \item $a \mapsto_a as$ holds if the heap at position $a$ is reserved and contains
    an array representing $as$.
    \item $a \mapsto_r x$ holds if the heap at position $a$ is reserved and contains
    value $x$ where $x$ is of some type $'a::heap$
    \item $\exists_A x.\ P\ x$ holds if there exists some $x$ such that predicate P
    holds on the heap for given $x$.
    \item $P_1 * P_2$ holds if each assertion holds on its part of the heap
    and the areas of the heap described by each assertion are non-overlapping
\end{itemize}

The last assertion is key to enabling reasoning about local
parts of the heap.
With the tools provided by separation logic,
we well prove our imperative implementations correct
by showing that they refine the abstract implementation in \Cref{chapter:abs-set}.

In general, refinement relates a concrete data structure
to an abstract structure via some refinement assertion.
Examples for such a refinement assertion are the id assertion and the
list assertion.
The latter relates lists, given a refinement
assertion between the elements of the first and second list.

\begin{lstlisting}[mathescape=true, language=Isabelle]
definition id_assn a b = $\uparrow$(a = b)

fun list_assn :: ('a $\Rightarrow$ 'c $\Rightarrow$ assn) $\Rightarrow$ 'a list $\Rightarrow$ 'c list $\Rightarrow$ assn where
  list_assn P [] [] = emp
| list_assn P (x#xs) (y#ys) = P x y * list_assn P xs ys
| list_assn _ _ _ = false
\end{lstlisting}

We can then specify Hoare Triples on assertions,
to encapsulate statements about imperative programs.

\begin{definition}{Hoare Triple}
    $\langle$ P $\rangle$ c $\langle \lambda r. $ Q r $\rangle$ holds iff \\
    if assertion $P$ holds on some heap before operation $c$
    is executed on the heap,
    and operation $c$ returns some $r$, then assertion $Q$ holds
    for $r$ and on the heap modified by $c$
\end{definition}

A simple example Hoare Triple is 
$\langle$ id\_assn $a$ $b$ $\rangle$ return $b$ $\langle \lambda r.$ id\_assn $a$ $r$ $\rangle$.
In the refinement process, we will use more complex relationship assertions
between some abstract $b$ and some refined $bi$.
We then show that the value $r$ returned
by the imperative program \textit{imp\_op} satisfies the same relationship
to \textit{op} $b$ where \textit{op} is the refined abstract specification.
Further we use the notation $\langle\ P\ \rangle\ c\ \langle \lambda r.\ Q\ r\ \rangle_t$
as a shorthand for $\langle\ P\ \rangle\ c\ \langle \lambda r.\ Q\ r\ * true\ \rangle$.
Since $true$ holds for any heap, this part of the assertion
may be used to subsume all temporary and discarded variables.

Refining all set operations from \Cref{chapter:abs-set}
will yield imperative code that operates on the heap
with the  option to use efficient destructive updates.
We do not need to show that the operations themselves
satisfy the requirements of the set interface.
Instead we merely need to show that the imperative code
acts on a concretized version of the B-trees
the same way as the abstract operations on the abstract version.

% TODO note what to introduce here
% abstract idea: implementation that has same effect on concrete ds
% as abstract function on abstract ds
% TODO short intro hoare triples and notation here
% *  and \uparrow

\subsection{Refinement of abstract lists}

In the imperative collection framework by Lammich
\parencite{DBLP:journals/jar/Lammich19},
the abstract datatype list is usually
refined to a dynamic array like data structure.
It comprises an array and a natural number,
where the latter denotes the current number of elements
stored in the array.
B-tree nodes do not always contain a constant number of elements,
which is why this property should be transferred to
the data structure refining key-value lists. 

% TODO intended physical storing of B-tree on pages of memory
% --> partially filled array, non-dynamic!
However, dynamic arrays are designed to grow and shrink
as elements are inserted and deleted.
B-trees were invented to be stored on slow hard disks.
As one disk access loads as much data as fits into a page of main memory,
nodes of roughly this size make the most efficient use of an access
\parencite{DBLP:journals/acta/BayerM72}.
%TODO give clear reason
Therefore the data structure to store the node content
should reserve arrays of fixed size, independent
of the actual number of keys and children contained.

Based on the definition of dynamic arrays in the 
imperative collection framework,
we introduce a simpler data structure,
the \textit{partially filled array},
that keeps count of currently inserted elements,
but does neither grow nor shrink.
A list $ls$ is represented by a partially filled array $(a,n)$
if the array $a$ represents a list $ls'$, of which the first $n$
elements form list $ls$.
Formally,

\begin{lstlisting}[mathescape=true, language=Isabelle]

datatype 'a pfarray = 'a array $\times$ nat

definition is_pfa :: nat $\Rightarrow$ 'a list $\Rightarrow$ 'a pfarray where
    is_pfa c ls (a,n) =
        $\exists_A$ ls'. a $\mapsto_a$ ls' *  $\uparrow$(c = length ls' $\wedge$ n $\le$ c $\wedge$ ls = (take n ls'))
\end{lstlisting}

where $c$ is the $capacity$ of the array,
the actually allocated size of the array on the heap.
All references to lists in the abstract datatype $btree$
will in the imperative version be refined to partially filled arrays.

However one should note that
from the set operation specification in \Cref{chapter:abs-set} alone
it is not determined which operations are supposed to be conducted
in-place and which will require copying data.
To obtain the most efficient code we therefore manually
define the operations in \Cref{lst:pfarray-def} to replace complex composite
operations based on list construction and catenation.

% abstract function has length, insertion, take/drop
% --> concrete implementations
\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle,label={lst:pfarray-def},
    caption={Important Partially Filled Array functions for Insertion.}]
definition pfa_length $\equiv$ $\lambda$(a,n). return n

definition pfa_get $\equiv$ $\lambda$(a,n) i. Array.nth a i (* returns the nth element in array a *)

definition pfa_set $\equiv$ $\lambda$(a,n) i x. do {
    Array.upd i x a; (* sets the element at position i in a to value x *)
    return (a,n)
}

definition pfa_shrink k $\equiv$ $\lambda$(a,n). return (a,k)

definition pfa_shrink_cap  k $\equiv$ $\lambda$(a,n). do {
    a' $\leftarrow$ array_shrink a k; (* returns an array with the given actual size, potentially reallocated*)
    return (a',min k n)
}

definition pfa_drop $\equiv$ $\lambda$(src,sn) si (dst,dn). do {
    blit src si dst 0 (sn-si);
    return (dst,(sn-si))
}

definition pfa_insert $\equiv$ $\lambda$(a,n) i x. do {
  array_shr a i 1; (* shifts elements from index i on to the right by 1 *)
  Array.upd i x a;
  return (a,n+1)
}

definition pfa_ensure $\equiv$ $\lambda$(a,n) k. do {
  a' $\leftarrow$ array_ensure a k default; (* returns an array with given minimal size, potentially reallocated *)
  return (a',n)
}

definition pfa_insert_grow  $\equiv$ $\lambda$(a,n) i x. do {
  a' $\leftarrow$ pfa_ensure (a,n) (n+1);
  a'' $\leftarrow$ pfa_insert a' i x;
  return a''
}

$\dots$
\end{lstlisting}
\end{figure}

% TODO also include what the rule gives us?

During the splitting of nodes, a part of the overflowing
list is copied into another array.
The operation to copy data is based on the function $blit$,
that copies a slice of an array to a slice of another array.
The function already existed in the standard collection and could be reused
to define the \textit{pfa\_drop} function in \Cref{lst:pfarray-def}, which in this implementation copies data to a new array.

The $blit$ function is however not capable of correctly copying data
within the array, as required for efficient in-place insertion and deletion.
A suitable function, called $sblit$, was hence added in the course of this project.
The relevant addition is to differentiate between copying elements
from higher to lower indices or from lower to higher indices.
Depending on the direction of the copy, it is relevant
in which order to copy singleton elements so that in case
of overlapping ranges no elements are overwritten before being copied.

By adding a function that copies elements in reverse order
and conditionally calling either copy function,
loss-free in-place copying was achieved.
The resulting function is conveniently agnostic to the direction
of the copying, just as the respective implementations
in target languages for code extraction.\footnote{
    See for example \href{https://caml.inria.fr/pub/docs/manual-ocaml/libref/Array.html}{the specification of \texttt{Array.blit} in Ocaml} or
    the \href{https://smlfamily.github.io/Basis/array-slice.html\#SIG:ARRAY_SLICE.copy:VAL}{\texttt{ArraySlice.copy} specification in SML}.
}
The \textit{pfa\_insert} function in \Cref{lst:pfarray-def} makes
implicit use of in-place copying by calling a function to shift elements
to the right by one.
This is achieved by copying the slice of elements
from position $i$ to the end of the list
to the slice $i+1$ to the end of the list.
The element to be inserted is then placed in the remaining free spot.

For all functions in \Cref{lst:pfarray-def},
appropriate Hoare Triples were derived.
All triples are straightforward to derive but the reader is encouraged
to look them up in the proof documents.
To roughly sketch the obtained lemmas, the two examples
\textit{pfa\_drop} and \textit{pfa\_shrink} are investigated below.
The most important difference is whether
the operations are destructive (in-place) or not.
For most operations, the approach that required the least
memory allocations was chosen.\footnote{
    The only exception is node$_i$, which in the case of overflow
    currently requires temporary allocation of an array that holds more than $2k$ elements.
}

\begin{lemma}
    $k \le \length s \wedge (\length s - k) \le dn \Longrightarrow$ \\
    \begin{center}
    $\langle \pfarraycap$ sn s si * $\pfarraycap$ dn d di $\rangle$ \\
    $\pfadrop$ si k di \\
    $\langle\lambda di'. \pfarraycap$ sn s si * $\pfarraycap$ dn ($\drop$ k s) di' $\rangle$
    \end{center}
\end{lemma}

The array $si$ stays untouched.
This fact is expressed by the fact that the term about its relationship to $s$
is still available in the postcondition of the triple.
The only thing that changed is the content of $di$.\footnote{
    In the actual proof, where possible, we even show that the array itself has not changed
    (i.e. has not been reallocated).
    The correct notation does however not yield high readability
    and is hence abstracted here.
}
The $take$ function was refined differently.
The cheapest and in our context consistent way to
implement $take$ is to
reduce the number of elements knowingly stored in the array
from $n$ to $k$.
The result is that the original array gets modified and we loose any information
about the content that is written beyond given $k$.

\begin{lemma}
    $k \le \length s\Longrightarrow$ \\
    \begin{center}
    $\langle \pfarraycap$ sn s si$\rangle$
    $\pfashrink$ si k
    $\langle\lambda si'. \pfarraycap$ sn ($\take$ k s) si' $\rangle$
    \end{center}
\end{lemma}

With these operations and suitable Hoare-Triples about them,
we have all the tools required to refine the abstract set implementation.
The only missing piece is an imperative split function.
% TODO more about sblit?

\section{Set-Operations}

Having covered the refinement for the list datatype,
the B-tree datatype itself can be refined.
Rather than actually storing nodes within lists,
the B-tree nodes are supposed to be stored in the heap,
and the node list will only contain pointers to the corresponding nodes.
Pointers are represented by the type \textit{ref} in Imperative/HOL.
Leaf nodes contain no information and consequently don't have to be stored
on the heap, therefore we refine them using null pointers.
Since the Imperative Refinement Framework does not support null pointers,
B-tree nodes are refined by heap reference \textit{options} instead.
None represents leafs or empty trees.
Valid heap references point to refinements of internal nodes.

\begin{lstlisting}[mathescape=true, language=Isabelle]
datatype 'a btnode =
    Btnode (('a btnode ref option*'a) pfarray) ('a btnode ref option)
\end{lstlisting}

Note the similarity to the definition in \Cref{lst:btree-def},
only that \textit{btree} was replaced by \textit{btnode ref option},
\textit{list} was replaced by \textit{pfarray},
and there is no leaf case anymore.

An abstract B-tree is represented by such a physical node
if all references on subtrees represent the corresponding
abstract subtrees and all keys are the same.
The precise refinement assertion can be seen in \Cref{lst:imp-btree-refine}.

\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, caption={The refinement assertion for B-trees},
    label={lst:imp-btree-refine}]
fun btree_assn :: nat $\Rightarrow$ 'a::heap btree $\Rightarrow$ 'a btnode ref option $\Rightarrow$ assn where
  btree_assn k Leaf None = emp |
  btree_assn k (Node ts t) (Some a) = 
 ($\exists_A$ tsi ti tsi'.
      a $\mapsto_r$ Btnode tsi ti
    * btree_assn k t ti
    * is_pfa (2*k) tsi' tsi
    * list_assn ((btree_assn k) $\times_a$ id_assn) ts tsi'
    ) |
  btree_assn _ _ _ = false

abbreviation blist_assn k $\equiv$ list_assn ((btree_assn k) $\times_a$ id_assn)

\end{lstlisting}
\end{figure}

It seems a little bit awkward that the definition
includes two relationships regarding only the list of elements in the subtree.
The reason is that there are two steps of refinements going on.
First, all elements in the list are refined to their imperative counterparts.
This fact is expressed by the \textit{list\_assn} term in the assumption.
Second, the list itself is refined to an array.
This is expressed in \textit{is\_pfa} term.
Since the list assumption makes recursive use of the
B-tree assertion, we again cover up usage of the term
by an abbreviation.

Using the parameters of \textit{is\_pfa},
note that we can specify that every node of a B-tree should have
a capacity of $2k$.

\subsection{The split function}
\label{sec:imp-split}

To implement any set operations we again need a split operation.
However, what split function exactly will be used can be abstracted again.
The only important thing is that it refines an abstract split function.
However we have some freedom on how to specify this refinement relationship.

Imperative split functions should be efficient and as such should not actually
split the array in half.
The operation should rather return the index of the correct
subtree-separator pair.
The refinement condition derived from this relationship is as follows.

\begin{lstlisting}[mathescape=true, language=Isabelle]
definition split_relation xs (ls,rs) i = i $\le$ length xs $\wedge$ ls = take i xs $\wedge$ rs = drop i xs
\end{lstlisting}

A very useful alternative statement of the relationship follows automatically.
This allows us to simplify terms that only contain $xs$
to the two sublists when we obtain this relationship in proofs.

\begin{lemma}
    $\splitrelation xs\ (ls,rs)\ i = (xs = ls@rs \wedge i = \length ls)$
\end{lemma}

From this, we characterize the desired imperative
split function \textit{imp\_split} by its Hoare-Triple.

\begin{align*}
    \langle & \pfarraycap c\ tsi\ a * \blistassn k\ ts\ tsi *  true \rangle \\
            & \impsplit a \\
\langle\lambda i. & \pfarraycap c\ tsi\ a * \blistassn k\ ts\ tsi * \uparrow(\splitrelation ts\ (\splitfun ts p)\ i)\rangle_t
\end{align*}

Just as with the abstract implementation,
the imperative refinements can be built with this characterization alone.
However this time we are actually interested in finding an
efficient split and will spend some time finding
a good split function.
%TODO move to after the set operations?

In the imperative context, we prefer not to directly
refine the recursive function from \Cref{fig:linear_split}
but we embark on the exercise of using a while loop.
The implementation in \Cref{lst:imp-linear-split}
not more efficient than the functional linear search,
but suitable for getting to know
the usage of while loops in imperative HOL.
Every iteration, an index on the array is moved forward by one.
The main part is done in the loop head - if we
are within the array bounds,
we obtain the current separator and check if it is
smaller than the partitioning element.
We return the first index at which the separator is greater or equal.

\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, caption={The imperative linear split},
    label={lst:imp-linear-split}]
definition lin_split :: ('a::heap $\times$ 'b::{heap,linorder}) pfarray $\Rightarrow$ 'b $\Rightarrow$ nat Heap where
    lin_split (a,n) p = do { 
        i $\leftarrow$ while  
            ($\lambda$i. if i<n then do { 
                (_,s) $\leftarrow$ Array.nth a i; 
                return (s<p) 
            } else return False)  
            ($\lambda$i. return (i+1))  
            0; 
        return i 
    }
\end{lstlisting}
\end{figure}

As mentioned, the internal behavior of this function is
quite different to the abstract linear split from \Cref{fig:linear_split}.
Therefore, proving that it refines the abstract function requires a small detour.
We first express and show the most basic, non-trivial statement
we can make about the result of the function.
We then show that the result is the same as the result
of applying the abstract function,
that is that to an external observer, the functions behave the same.

We begin by stating a simple matching Hoare Triple
for the function.

\begin{lemma}
    \label{lem:lin-split-ht}
    \begin{align*}
        \langle &\pfarraycap c\ xs\ (a,n)\rangle \\
                  &\linsplit\ (a,n)\ p \\
        \langle \lambda i. & \pfarraycap c\ xs\ (a,n) \\
        * &\uparrow(i \leq n 
            \wedge (\forall j < i.\ snd (xs!j) < p) 
            \wedge (i<n \longrightarrow snd (xs!i) \geq p)) \rangle_t 
    \end{align*}
\end{lemma}

It follows by supplying the following loop invariant:

\begin{itemize}
    \item $\forall j<i.\ \snd (xs!j) < p$: All elements up to the current $i$
        are smaller than the partitioning element,
    \item $\uparrow(i \leq n)$: $i$ does not exceed the length of the array and
    \item $\pfarraycap c\ xs\ (a,n)$: the array is not manipulated by the operation
\end{itemize}

In order to show that the loop also terminates,
we need to provide a measure that strictly decreases in each iteration.
The measure for the loop is the difference $n-i$ which
decrements by one in each iteration and stays positive.

For unexciting reasons, the post condition we have thus obtained
actually suffices to imply that the split relation
to the result of an abstract split function is satisfied.
Specifically, we use the abstract split function from
\Cref{fig:linear_split}. %TODO or explain why a different function was used
For the two cases of loop termination,
either $i = n$, or $i < n$, we find that
the abstract function will return the exact same split.

This is great to know since the previous lemma
follows directly from the computation and is closely related
to the loop invariant.
This will simplify proofs for alternative imperative split functions.
\footnote{
    Another, albeit more theoretical,
    reason for excitement is that we can now specify imperative B-tree
    interfaces without any reference to the abstraction.
    We simply always use the abstract linear split function
    and only require that the imperative split function satisfies
    the hoare triple from \Cref{lem:lin-split-ht}.
}
With this reassurance we approach the binary split.

A detailed analysis of binary search algorithms
may be found in the work of Montague \parencite{DBLP:journals/csedu/Montague91},
which has served as an orientation
for the derived algorithm in \Cref{lst:imp-binary-split}.
Rather than walking through the array,
it narrows the range in which the desired element may lie
to a window that it bisected in every iteration.
This bisection is implemented by inspecting the middle element of the window.
If the desired element was not found, the algorithm
recurses into either the left or right remaining subwindow.
In the final iteration,
the window has narrowed down to a single element,
containing the correct separator and subtree.

\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, caption={The imperative binary split},
    label={lst:imp-binary-split}]
definition bin_split :: ('a::heap $\times$ 'b::{heap,linorder}) pfarray $\Rightarrow$ 'b $\Rightarrow$ nat Heap 
    where bin_split (a,n) p = do { 
        (low',high') $\leftarrow$ while  
            ($\lambda$(low,high). return (low < high))  
            ($\lambda$(low,high).
            let mid = ((low  + high) div 2) in 
            do { 
                (_,s) $\leftarrow$ Array.nth a mid; 
                if p < s then 
                    return (low, mid) 
                else if p > s then 
                    return (mid+1, high) 
                else
                    return (mid,mid) 
            })  
            (0::nat,n); 
        return low' 
    }
\end{lstlisting}
\end{figure}

Since pairs as list elements (i.e. pairs of trees and separators)
were expected to add another layer of complexity,
a binary split algorithm on normal lists has been derived first.
The code required several small corrections
until it could be proven correct.
This supports the general call
for verified programs and hints at the fragility
of efficient unverified programs.
The split function for lists of pairs was then defined
and could be tackled using the same loop invariant.
Note that the binary split works on two indices
as states, the upper bound $h$ and the lower bound $l$.

\begin{itemize}
    \item $\forall j<l.\ \snd (xs!j) < p$: All elements up to the current $l$
        are smaller than the partitioning element,
    \item $h<n \rightarrow \snd (xs!h) < p$: If the upper bound $h$
    is strictly less than the length of the array,
    the element it points to is greater than the partioning element,
    \item $\uparrow(l \leq h)$: The lower bound is always less or equal to the upper bound,
    \item $\uparrow(h \leq n)$: the upper bound does not exceed the length of the array and
    \item $\pfarraycap c\ xs\ (a,n)$: again, the array is not manipulated by the operation
\end{itemize}

In this case the window that is considered by the binary split
provides the loop measure, as $h-l$ decreases by at least one in each step.
Proving that this is the case was not the main issue,
but rather finding a correct formulation of the algorithm
where this is actually true.

Overall obtaining the same Hoare Triple
as for the linear split required only
this slightly more complex loop invariant
and invocation of a number of algebraic lemmas.
This was somewhat of a surprise, since the functional
version of a binary split is much harder to
specify and analyse than a functional linear split.
The additional requirement for the binary split is
naturally that the list of separators must be sorted.

\begin{lemma} $\sorted (\separators xs) \Longrightarrow$ \\
    \label{lem:imp-bin-split-ht}
    \begin{align*}
        \langle &\pfarraycap c\ xs\ (a,n)\rangle \\
                  &\binsplit\ (a,n)\ p \\
        \langle \lambda i. & \pfarraycap c\ xs\ (a,n)\\
        &* \uparrow(i \leq n 
            \wedge (\forall j < i. snd (xs!j) < p) 
            \wedge (i<n \longrightarrow snd (xs!i) \geq p)) \rangle_t 
    \end{align*}
\end{lemma}

Since we know that this implies equivalence to the abstract split function
and sortedness of the separators is guaranteed by the sortedness
invariant of the B-trees,
we can safely use this function for specifying
the set operations.

%TODO Note the choice of a while-loop for efficiency
%TODO The split function can be implemented linearly (easier, proof of concept)
% but due to abstraction of functionality may use binary split as well.

\subsection{The isin function}

In general, the formulation of the imperative set operations
do quite directly follow
from the refinement of the abstract operations.
The main difference is the usage of pointers,
that need to be dereferenced and updated.
Functions such as length need to be called and have their result
stored explicitly before use.
Other than that, the definition of the imperative isin in \Cref{lst:imp-isin-fun}
function should bare no surprises.
Note that it makes use of the function \textit{imp\_split}
which is an arbitrary split function that fulfils the Hoare Triple
of \Cref{lem:imp-bin-split-ht}.

\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, label={lst:imp-isin-fun},
    caption={The imperative isin function}]
partial_function (heap) isin :: 'a btnode ref option $\Rightarrow$ 'a $\Rightarrow$  bool Heap
  where
    isin p x =
  (case p of
     None $\Rightarrow$ return False |
     (Some a) $\Rightarrow$ do {
       node $\leftarrow$ !a;
       i $\leftarrow$ imp_split (kvs node) x;
       tsl $\leftarrow$ pfa_length (kvs node);
       if i < tsl then do {
         s $\leftarrow$ pfa_get (kvs node) i;
         let (sub,sep) = s in
         if x = sep then
           return True
         else
           isin sub x
       } else
           isin (last node) x
    }
)
\end{lstlisting}
\end{figure}

The convention for variables that refer to refined datatypes
is that they have the same name as the abstract datatype,
with the postfix \textit{i}.
For example below, the abstract \textit{btree} is referred to by $t$,
however the refined \textit{btnode ref option} is called $ti$.
To simplify notation, we simply assume that the set operations
such as \textit{isin} are overloaded and refer to abstract or
imperative implementations, depending on the parameter type.
The Hoare Triple to argue that the imperative \textit{isin}
refines the abstract \textit{isin} follows.

\begin{lemma} $\sorted (\inorder t) \Longrightarrow$ \\
\begin{align*}
   \langle &\btreeassn k\ t\ ti\rangle \\
           &\isinfun ti\ x \\
   \langle \lambda r. &\btreeassn k\ t\ ti\ * \uparrow(\isinfun t\ x = r)\rangle_t
\end{align*}
\end{lemma}

Note how, in contrast to the abstract specification,
we need to also show that the imperative function does not modify the tree
residing in the heap.
The proof follows by induction on the computation of the abstract isin function.

% TODO conclusion ?
Inside a structured proof skeleton of the induction,
proving the subgoals in apply style turned out to be the most
appropriate.
The separation automation tool only required help
for the instantiation of existential quantifiers,
especially after application of the inductive hypothesis. 
% TODO more details?

\subsection{Insertion}
% TODO...
\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, label={lst:imp-nodei-fun},
    caption={The imperative node$_i$ function}]
definition node$_i$ 
    :: nat $\Rightarrow$ ('a btnode ref option $\times$ 'a) pfarray $\Rightarrow$ 'a btnode ref option $\Rightarrow$ 'a btupi Heap
  where node$_i$ k a ti $\equiv$ do {
      n $\leftarrow$ pfa_length a;
      if n $\le$ 2*k then do {
        a' $\leftarrow$ pfa_shrink_cap (2*k) a;
        l $\leftarrow$ alloc (Btnode a' ti);
        return (T$_i$ (Some l))
      }
      else do {
        b $\leftarrow$ (pfa_empty (2*k)
            :: ('a btnode ref option $\times$ 'a) pfarray Heap);
        i $\leftarrow$ split_half a;
        m $\leftarrow$ pfa_get a i;
        b' $\leftarrow$ pfa_drop a (i+1) b;
        a' $\leftarrow$ pfa_shrink i a; 
        a'' $\leftarrow$ pfa_shrink_cap (2*k) a';
        let (sub,sep) = m in do {
          l $\leftarrow$ alloc (Btnode a'' sub);
          r $\leftarrow$ alloc (Btnode b' ti);
          return (Up$_i$ (Some l) sep (Some r))
        }
      }
    }

\end{lstlisting}
\end{figure}

\begin{figure}
\begin{lstlisting}[mathescape=true, language=Isabelle, label={lst:imp-ins-fun},
    caption={Excerpt of the imperative insert function}]

partial_function (heap) ins :: nat $\Rightarrow$ 'a $\Rightarrow$ 'a btnode ref option $\Rightarrow$ 'a btupi Heap 
    where 
      ins k x apo = (case apo of 
    None $\Rightarrow$ return (Up$_i$ None x None) | 
    (Some ap) $\Rightarrow$ do { a $\leftarrow$ !ap; i $\leftarrow$ imp_split (kvs a) x; tsl $\leftarrow$ pfa_length (kvs a); 
      if i < tsl then do { 
        s $\leftarrow$ pfa_get (kvs a) i; 
        let (sub,sep) = s in 
        if sep = x then return (T$_i$ apo) 
        else do { 
          r $\leftarrow$ ins k x sub; 
          case r of  
            (T$_i$ lp) $\Rightarrow$ do { pfa_set (kvs a) i (lp,sep); return (T$_i$ apo) 
            } | 
            (Up$_i$ lp x' rp) $\Rightarrow$ do { 
              pfa_set (kvs a) i (rp,sep); 
              if tsl < 2*k then do { 
                  ts' $\leftarrow$ pfa_insert (kvs a) i (lp,x'); 
                  ap := (Btnode ts' (last a)); 
                  return (T$_i$ apo) 
              } else do { 
                ts' $\leftarrow$ pfa_insert_grow (kvs a) i (lp,x'); 
                node$_i$ k ts' (last a) 
        } } } } else $\dots$

definition insert :: nat $\Rightarrow$ ('a::{heap,default,linorder}) $\Rightarrow$ 'a btnode ref option $\Rightarrow$ 'a btnode ref option Heap
 where insert $\equiv$ $\lambda$ k x ti. do {
  ti' $\leftarrow$ ins k x ti;
  case ti' of
     T$_i$ sub $\Rightarrow$ return sub |
     Up$_i$ l a r $\Rightarrow$ do {
        ts' $\leftarrow$ pfa_init (2*k) (l,a) 1;
        t' $\leftarrow$ alloc (Btnode ts' r);
        return (Some t')
      }
}

\end{lstlisting}
\end{figure}

%TODO Description of applying the refinement framework to the insert function
The imperative implementation and derived heap rule of the \textit{insert} function follow
the same pattern as the \textit{isin} function.
The only interesting part is whether and how often new memory is allocated.
An excerpt of the refinement of the \textit{ins} function
can be seen in \Cref{lst:imp-ins-fun}\footnote{
    In the imperative language in Isabelle, allocating heap memory
    is abbreviated by \textit{ref}.
    To clarify that this means allocation, it is named \textit{alloc} here.
}.
The function includes some simple optimizations, such as not calling
\textit{node$_i$} if the node is not overflowing.
This is an optimization as otherwise \textit{node$_i$} in
\Cref{lst:imp-nodei-fun} would always allocate a new B-tree node
rather than updating the current node in place.

To prove that this optimization is valid, the only obligation
is to show that the updated node itself is equivalent
to the node returned by \textit{node$_i$}.
Since we are in the branch where the node contains less than $2k$
elements, this is easy to show, provided some intermediate lemmas
that relate concrete node and abstract node in this context.

\begin{lemma}$\length ts \leq 2k \Longrightarrow
    \node_i k\ ts\ t = T_i (Node\ ts\ t)$
\end{lemma}

\begin{samepage}
By adding a lemma on the equivalence between abstract and imperative
\textit{node$_i$}, we obtain a lemma for \textit{ins}
by induction on the computation
of the abstract \textit{ins} function.

\begin{lemma} $\sorted (\inorder t) \Longrightarrow$ \\
    \begin{align*}
       \langle &\btreeassn k\ t\ ti\rangle \\
               &\insfun k ti\ x \\
       \langle \lambda r. &\btreeassn k\ (\insfun k\ t\ x) r\rangle_t
    \end{align*}
\end{lemma}
\end{samepage}

The proof of this lemma was done in a mixed structural and apply style manner.
It could have been done in a complete apply style manner.
However many subgoals are generated in the proof process that are
simply invalid due to the preconditions of the hoare triple.
Embedding the proof in the structural skeleton makes spotting
these void statements easier and allows to have a better
overview over the current state of the proof.

The insertion function incorporates the function \textit{tree}$_i$
in much the same way as \textit{ins} incorporated the small \textit{node}$_i$ optimizations.
Here the proof of equivalence followed automatically.
With the hoare triple for \textit{ins}, we obtain the final desired lemma
on the imperative insertion.

\begin{theorem} $k > 0 \wedge \sorted (\inorder t) \Longrightarrow$ \\
\begin{align*}
   \langle &\btreeassn k\ t\ ti\rangle \\
           &\insertfun k\ x\ ti \\
   \langle \lambda ri. &\btreeassn k\ (\insertfun k\ x\ t)\ ri\rangle_t
\end{align*}
\label{thm:imp-ins-rule}
\end{theorem}

% TODO say something about k?
Note how we have the additional assumption that $k$ is positive.
This was only required to show correct order in the abstract setting.
Here, the capacity of the root node needs to be specified at initialization.
And since the capacity has to be $2k$ and the node has to hold at least one element
a positive $k$ is required such that no contradiction appears.

Until now we have only shown that the concrete
implementations refine the abstract interpretations.
What does this bring concretely?
Do we have to finally resort to an informal argument to show that the result
satisfies the invariants and the set interface?
This is not the case.
The final theorem that proves functional correctness of the imperative
implementations with respect to the set interface follows.

\begin{corollary} $k > 0 \Longrightarrow$
\begin{align*}
   \langle &\btreeassn k\ t\ ti \wedge \btree_k t \rangle \\
           &\insertfun k\ x\ ti \\
   \langle \lambda ri. \exists_A r. &\btreeassn k\ r\ ri \wedge \btree_k r \wedge \inorder r = \inslist x\ (\inorder t)\rangle_t
\end{align*}
\end{corollary}

And indeed, this can be derived automatically with a single command invocation,
unfolding \Cref{def:btree-def} and using \Cref{thm:imp-ins-rule}, \Cref{thm:ins-set} and \Cref{thm:ins-invar}.\footnote{
    Technically we also need the fact that ins$_{list}$ preserves sortedness.
    This was already proven by Nipkow \parencite{DBLP:conf/itp/Nipkow16}
    and is considered trivial here.
}


\subsection{Deletion}

An imperative version of the deletion operation has been specified.
It required some additional operations on partially filled arrays but
posed no foundational challenges.
Due to its length it is not shown here but may be looked up in the
actual proof documents.

However the proof of its refining property was not conducted.
We assume that this proof would be structured similar to
the previous proofs.
The only benefit would be exploring usage of the new
heap rules introduced for the additional array operations.
The main difference to the verification of the
insertion function is that the refined functions
are undefined for many cases.
Hence it is highly important
to strongly tie the refined and imperative version
in order to discharge void cases.